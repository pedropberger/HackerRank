# -*- coding: utf-8 -*-
"""NLP-MPES-PI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1baLN08sLGLFQRaOcpkjSio_qlt74-4BY

NLP MPES PI

Roadmap

- Import data
- ETL
- Html clean
- Token
- Embedding
- Relation matrix

# Packages
"""

# Data
import pandas as pd
from collections import Counter
import matplotlib.pyplot as plt

# Tools
from google.colab import drive
import os
from bs4 import BeautifulSoup

# NLP

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
import re

"""# Upload and Clean Data"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/NLP_PI_Beta'

# Load the data into a DataFrame

db_full =  pd.read_parquet(file_path)

# Now you can work with the data
print(db_full.head())  # Display the first few rows of the data

"""# HTML Cleaning"""

# Fase 2 - Cleaning HTML

def extract_text_from_html(html):
    soup = BeautifulSoup(html, 'html.parser')

    # Add space between words (after we will remove then in the tokenization fase)
    text_with_spaces = ' '.join(soup.stripped_strings)

    return text_with_spaces

# Aplicando a função à coluna 'conteudo_html'
db_full['conteudo_text'] = db_full['conteudo_html'].apply(extract_text_from_html)

# Exibindo o DataFrame resultante
print(db_full.head())

print('Dropando a coluna "conteudo_html"')
db_full.drop('conteudo_html', axis=1, inplace=True)

db_full.head()

print(db_full.loc[1, 'conteudo_text'])

"""# Tokenizer"""

# Downloading packs

nltk.download('stopwords')
nltk.download('punkt')

# Função para tokenizar o texto removendo stopwords
def tokenize_text(text):
    tokens = word_tokenize(text.lower())  # Tokenização e conversão para minúsculas

    #tokens = [token for token in tokens if token.isalpha()]  # Remover pontuações e números
    stop_words = set(stopwords.words('portuguese')) # Remover pontuações e números
    tokens = [token for token in tokens if token not in stop_words]  # Remover stopwords

    # Identificar padrões de "Lei nº número/ano xx.xxx/xxxx|xxxxx/xx||" e combinar o número com a palavra "lei"
    for i, token in enumerate(tokens):
        if token == "lei" and i < len(tokens) - 2 and re.match(r'\d{1,5}\.\d{3}/\d{4}|\d{1,5}/\d{1,4}', tokens[i+2]):
            numero_lei = re.sub(r'\D', '', tokens[i+2])  # Extrair apenas os dígitos do número da lei
            tokens[i] = f"lei{numero_lei}"
            del tokens[i+1:i+3]  # Remover os tokens "nº" e o número/ano da lei

    # Identificar padrões de "Lei número/ano xx.xxx/xxxx|xxxxx/xx||" e combinar o número com a palavra "lei"
    for i, token in enumerate(tokens):
        if token == "lei" and i < len(tokens) - 1 and re.match(r'\d{1,5}\.\d{3}/\d{4}|\d{1,5}/\d{1,4}', tokens[i+1]):
            numero_lei = re.sub(r'\D', '', tokens[i+1])  # Extrair apenas os dígitos do número da lei
            tokens[i] = f"lei{numero_lei}"
            del tokens[i+1:i+2]  # Remover os tokens "nº" e o número/ano da lei


    # Remover pontuações e símbolos avulsos
    tokens = [re.sub(r'[^\w\s]', '', token) for token in tokens]

    # Remover tokens vazios
    tokens = [token for token in tokens if token.strip()]

    return tokens


string = "A Lei Nº 12.527/2011 e a lei nº 2527/11 e a lei 2527/11 e a lei n 27/2017 são maravilhosas e bacanas."
token_test = tokenize_text(string)
print(token_test)

# Aplicar a função de tokenização à coluna de texto
db_full['token'] = db_full['conteudo_text'].apply(tokenize_text)

# Exibir o DataFrame resultante
db_full.head()

print(db_full.loc[1, 'token'])

# Tokenize the text (assuming it's already tokenized, otherwise you can use NLTK or SpaCy for tokenization)
tokens = [token for sublist in db_full['token'] for token in sublist]

# Count the frequency of each token
token_freq = Counter(tokens)

print((token_freq))

# export token_freq as xlsx with colums token and frequency

xls_path = '/content/drive/My Drive/Colab Notebooks/Data/token_freq_full.xlsx'

# Create a DataFrame from the token frequencies
token_freq_df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])

# Save the DataFrame to an Excel file
token_freq_df.to_excel(xls_path, index=False)

# Visualize the token frequencies (optional)
most_common_tokens = token_freq.most_common(20)  # Select the 20 most common tokens
tokens, frequencies = zip(*most_common_tokens)  # Unpack tokens and frequencies

print(most_common_tokens)

"""# Token Cleasing"""

# Create a df called db_token from db_full with only the collums idauto and token

db_token = db_full[['IdAuto', 'token']]

db_token.head()

# Remove tokens with low frequency

freq_min = 50

# Get the list of tokens with frequency less than freq
low_freq_tokens = [token for token, freq in token_freq.items() if freq < freq_min]

# Convert low_freq_tokens to a set for faster membership testing
low_freq_tokens_set = set(low_freq_tokens)

# Function to remove low-frequency tokens
def remove_low_freq_tokens(tokens):
    return [token for token in tokens if token not in low_freq_tokens_set]

# Apply the function to each row in 'token' column
db_token.loc[:, 'token'] = db_token['token'].apply(remove_low_freq_tokens)

# Print the updated DataFrame
db_token.head()

# Remove short tokens

token_short_len = 3

# Function to remove short tokens
def remove_short_tokens(tokens):
    return [token for token in tokens if len(token) >= token_short_len]

# Apply the function to each row in 'token' column
db_token.loc[:, 'token'] = db_token['token'].apply(remove_short_tokens)

# Print the updated DataFrame
print(db_token.head())

# Tokenize the text (assuming it's already tokenized, otherwise you can use NLTK or SpaCy for tokenization)
tokens = [token for sublist in db_token['token'] for token in sublist]

# Count the frequency of each token
token_freq = Counter(tokens)

print(len(token_freq))

len(db_token)

# Export data

output_path = '/content/drive/My Drive/Colab Notebooks/Data/NLP_PI_Beta_Token'

db_token.to_parquet(output_path)

# export token_freq as xlsx with colums token and frequency

xls_path = '/content/drive/My Drive/Colab Notebooks/Data/token_freq_fase1.xlsx'

# Create a DataFrame from the token frequencies
token_freq_df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])

# Save the DataFrame to an Excel file
token_freq_df.to_excel(xls_path, index=False)

"""# Token Cleasing 2"""

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/NLP_PI_Beta_Token'

# Load the data into a DataFrame

db_token =  pd.read_parquet(file_path)

db_token.head()

# Função para remover tokens que possuem apenas números
def remove_numeric_tokens(tokens):
    return [token for token in tokens if not re.match("^\d+$", token)]

# Aplicando a função ao dataframe
db_token['token'] = db_token['token'].apply(remove_numeric_tokens)

# Contar o número de tokens em cada campo da coluna 'token'
db_token['num_tokens'] = db_token['token'].apply(len)

# Exibir o DataFrame com a contagem de tokens
print(db_token)

db_token['num_tokens'].mean()

# Tokenize the text (assuming it's already tokenized, otherwise you can use NLTK or SpaCy for tokenization)
tokens = [token for sublist in db_token['token'] for token in sublist]

# Count the frequency of each token
token_freq = Counter(tokens)

# export token_freq as xlsx with colums token and frequency

xls_path = '/content/drive/My Drive/Colab Notebooks/Data/token_freq_fase_2.xlsx'

# Create a DataFrame from the token frequencies
token_freq_df = pd.DataFrame(token_freq.items(), columns=['token', 'frequency'])

# Save the DataFrame to an Excel file
token_freq_df.to_excel(xls_path, index=False)

"""# TF-IDF"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

import numpy as np

# prompt: create a TFIDF matrix from db_token['token']

from sklearn.feature_extraction.text import TfidfVectorizer

# Convert the 'token' column to a collection of strings
db_token.loc[:, 'text'] = db_token['token'].apply(lambda x: ' '.join(x))

# Fit the TF-IDF vectorizer on the converted 'token' column
vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=50, max_df=0.50,
                             stop_words=stopwords.words('portuguese'),
                             analyzer='word',
                             ngram_range=(1, 1),
                             lowercase=True,
                             use_idf=True)
X = vectorizer.fit_transform(db_token['text']).toarray()

print(X)

# Create a new DataFrame to store results
df_tfidf = pd.DataFrame(X, columns=vectorizer.get_feature_names_out())

# Add the "idauto" column from the original DataFrame
df_tfidf["idauto"] = db_token["IdAuto"]

# Now 'df_tfidf' contains the TF-IDF matrix and the "idauto" column

print(df_tfidf.head())

"""## Modelagem

### Regressão Logística - ODS 1
"""

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/db_model'

# Load the data into a DataFrame

db_model =  pd.read_parquet(file_path)

print(db_model.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Seleção das colunas
db_model = db_model.drop(columns=['ODS_2', 'ODS_3', 'ODS_4', 'ODS_5', 'ODS_6', 'ODS_7', 'ODS_8', 'ODS_9', 'ODS_10', 'ODS_11', 'ODS_12', 'ODS_13', 'ODS_14', 'ODS_15', 'ODS_16', 'ODS_17'])

# Separando variáveis e target
X = db_model.drop('ODS_1', axis=1)
y = db_model['ODS_1']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo de regressão logística
modelo = LogisticRegression()
modelo.fit(X_train, y_train)

# Fazendo previsões no conjunto de teste
y_pred = modelo.predict(X_test)

# Avaliando a acurácia do modelo
acuracia = accuracy_score(y_test, y_pred)
print(f"Acurácia do modelo: {acuracia:.2f}")

"""### Regressão Logística - ODS 3"""

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/db_model'

# Load the data into a DataFrame

db_model =  pd.read_parquet(file_path)

print(db_model.head())

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

# Seleção das colunas
db_model = db_model.drop(columns=['ODS_2', 'ODS_1', 'ODS_4', 'ODS_5', 'ODS_6', 'ODS_7', 'ODS_8', 'ODS_9', 'ODS_10', 'ODS_11', 'ODS_12', 'ODS_13', 'ODS_14', 'ODS_15', 'ODS_16', 'ODS_17'])

# Separando variáveis e target
X = db_model.drop('ODS_3', axis=1)
y = db_model['ODS_3']

# Dividindo em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Treinando o modelo de regressão logística
modelo = LogisticRegression()
modelo.fit(X_train, y_train)

# Fazendo previsões no conjunto de teste
y_pred = modelo.predict(X_test)

# Avaliando a acurácia do modelo
acuracia = accuracy_score(y_test, y_pred)
print(f"Acurácia do modelo: {acuracia:.2f}")

"""### SVM - ODS 3"""

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/db_model'

# Load the data into a DataFrame

db_model =  pd.read_parquet(file_path)

# Seleção das colunas
db_model = db_model.drop(columns=['ODS_2', 'ODS_1', 'ODS_4', 'ODS_5', 'ODS_6', 'ODS_7', 'ODS_8', 'ODS_9', 'ODS_10', 'ODS_11', 'ODS_12', 'ODS_13', 'ODS_14', 'ODS_15', 'ODS_16', 'ODS_17'])

# Importando as bibliotecas necessárias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

# Supondo que db_model é o seu dataframe e que ele já está carregado no ambiente
# Se não estiver carregado, você pode carregá-lo utilizando pd.read_csv ou outro método adequado
# db_model = pd.read_csv('seu_arquivo.csv')

# Separando as variáveis independentes (X) e a variável dependente (y)
X = db_model.drop(columns=['ODS_3'])
y = db_model['ODS_3']

# Dividindo os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Padronizando os dados (SVM é sensível à escala dos dados)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Criando e treinando o modelo SVM
svm_model = SVC(kernel='linear', random_state=42)  # Você pode experimentar outros kernels como 'rbf', 'poly', etc.
svm_model.fit(X_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = svm_model.predict(X_test)

# Avaliando o modelo
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Exibindo os coeficientes do modelo, se o kernel for linear
if svm_model.kernel == 'linear':
    coef = svm_model.coef_
    intercept = svm_model.intercept_
    print("Coeficientes:", coef)
    print("Intercept:", intercept)

"""### XGBoost - ODS 3"""

# Assuming your data file is a CSV file, change the format accordingly if it's different
file_path = '/content/drive/My Drive/Colab Notebooks/Data/db_model'

# Load the data into a DataFrame

db_model =  pd.read_parquet(file_path)

# Seleção das colunas
db_model = db_model.drop(columns=['ODS_2', 'ODS_1', 'ODS_4', 'ODS_5', 'ODS_6', 'ODS_7', 'ODS_8', 'ODS_9', 'ODS_10', 'ODS_11', 'ODS_12', 'ODS_13', 'ODS_14', 'ODS_15', 'ODS_16', 'ODS_17'])

# Importando as bibliotecas necessárias
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Supondo que db_model é o seu dataframe e que ele já está carregado no ambiente
# Se não estiver carregado, você pode carregá-lo utilizando pd.read_csv ou outro método adequado
# db_model = pd.read_csv('seu_arquivo.csv')

# Separando as variáveis independentes (X) e a variável dependente (y)
X = db_model.drop(columns=['ODS_3'])
y = db_model['ODS_3']

# Dividindo os dados em conjuntos de treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Padronizando os dados (embora não seja estritamente necessário para XGBoost, pode ajudar)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Criando e treinando o modelo XGBoost
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
xgb_model.fit(X_train, y_train)

# Fazendo previsões nos dados de teste
y_pred = xgb_model.predict(X_test)

# Avaliando o modelo
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Importância das features
feature_importances = xgb_model.feature_importances_
print("Importância das Features:", feature_importances)